{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml\n",
    "%pip install unidecode\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import logging\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retornar para pasta raiz, assumindo execução da pasta \"./src\". Se não for o caso alterar para os.chdir(pasta_raiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"RESULT\"):\n",
    "    shutil.rmtree(\"RESULT\") #Remove result folder and all files in it.\n",
    "os.makedirs(\"RESULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "f_handler = logging.FileHandler('./RESULT/logs.log', mode='w')\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "f_format = logging.Formatter(\"%(levelname)s: %(message)s\")\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error():\n",
    "    with open('RESULT/logs.log', 'r') as f:\n",
    "        log_content = f.read()\n",
    "    if 'ERROR' in log_content:\n",
    "        logger.critical(\"Execution interrupted. Adjust informed erros\")\n",
    "        f.close()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files = {\n",
    "    \"gli\": {\"field\": [\"LEIA\",\"ESCREVA\"]},\n",
    "    \"busca\": {\"field\": [\"MODELO\", \"CONSULTAS\", \"RESULTADOS\"]},\n",
    "    \"pc\": {\"field\": [\"LEIA\", \"CONSULTAS\", \"ESPERADOS\"]}, #read only once\n",
    "    \"index\": {\"field\": [\"LEIA\",\"ESCREVA\"]}  #read only once\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"gli.cfg\"\n",
    "logger.info(\"Reading config file \" + cfg_file)\n",
    "\n",
    "with open(os.path.join(\"config_files\", cfg_file), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    fields = [config_files[name]['field'] for name in config_files if cfg_file.endswith(name + '.cfg')][0]\n",
    "    gli_read = []\n",
    "    for line in lines:\n",
    "    # check if line has format \"field=file_path\"\n",
    "        field, file_path = line.strip().split('=')\n",
    "        # if field is 'LEIA', add file_path to gli_read\n",
    "        if field == 'LEIA':\n",
    "            gli_read.append(file_path)\n",
    "            # check if file_path exists\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.error(f\"Error: {file_path} doesn't exist\")\n",
    "        # if field is 'ESCREVA', check that it is the last field\n",
    "        elif field == 'ESCREVA':\n",
    "            if field not in fields[-1]:\n",
    "                logger.error(f\"Error: {field} field should be the last field in {cfg_file}\")\n",
    "            gli_write = file_path\n",
    "        # if field is not 'LEIA' or 'ESCREVA', print error\n",
    "        elif field not in fields:\n",
    "            logger.error(f\"Error: {field} is not a valid field in {cfg_file}\")\n",
    "    # check that all LEIA files exist\n",
    "    for leia_file in gli_read:\n",
    "        if not os.path.exists(leia_file):\n",
    "            print(f\"Error: {leia_file} doesn't exist\")\n",
    "    # check that all fields are present\n",
    "    if set(fields) != set(['LEIA', 'ESCREVA']):\n",
    "        print(f\"Error: {cfg_file} should have only 'LEIA' and 'ESCREVA' fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"busca.cfg\"\n",
    "logger.info(\"Reading config file \" + cfg_file)\n",
    "\n",
    "busca_model = None\n",
    "busca_queries = None\n",
    "busca_results = None\n",
    "\n",
    "with open(os.path.join(\"config_files\", cfg_file), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    fields = [config_files[name]['field'] for name in config_files if cfg_file.endswith(name + '.cfg')][0]\n",
    "    for line in lines:\n",
    "        # check if line has format \"field=file_path\"\n",
    "        field, file_path = line.strip().split('=')\n",
    "        # if field is 'LEIA', add file_path to gli_read\n",
    "        if field == 'MODELO':\n",
    "            if busca_model != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            busca_model = file_path\n",
    "            \n",
    "        # if field is 'ESCREVA', check that it is the last field\n",
    "        elif field == 'CONSULTAS':\n",
    "            if busca_queries != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            busca_queries = file_path\n",
    "\n",
    "        elif field == 'RESULTADOS':\n",
    "            if busca_results != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            busca_results = file_path\n",
    "\n",
    "        # if field is not correct, log error\n",
    "        elif field not in fields:\n",
    "            logger.error(f\"Error: {field} is not a valid field in {cfg_file}\")\n",
    "    # check that all fields are present\n",
    "    if set(fields) != set(['MODELO', 'CONSULTAS','RESULTADOS']):\n",
    "        logger.error(f\"Error in {cfg_file} adjust fields accordingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"index.cfg\"\n",
    "logger.info(\"Reading config file \" + cfg_file)\n",
    "\n",
    "index_read = None\n",
    "index_write = None\n",
    "\n",
    "with open(os.path.join(\"config_files\", cfg_file), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    fields = [config_files[name]['field'] for name in config_files if cfg_file.endswith(name + '.cfg')][0]\n",
    "    for line in lines:    \n",
    "    # check if line has format \"field=file_path\"\n",
    "        field, file_path = line.strip().split('=')\n",
    "        \n",
    "        if field == 'LEIA':\n",
    "            if index_read != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            index_read = file_path\n",
    "        \n",
    "        elif field == 'ESCREVA':\n",
    "            # if field is 'ESCREVA', check that it is the last field\n",
    "            if field not in fields[-1]:\n",
    "                logger.error(f\"Error: {field} field should be the last field in {cfg_file}\")\n",
    "            if index_write != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            index_write = file_path\n",
    "        # if field is not 'LEIA' or 'ESCREVA', print error\n",
    "        elif field not in fields:\n",
    "            logger.error(f\"Error: {field} is not a valid field in {cfg_file}\")\n",
    "    # check that all LEIA files exist\n",
    "    for leia_file in gli_read:\n",
    "        if not os.path.exists(leia_file):\n",
    "            logger.error(f\"Error: {leia_file} doesn't exist\")\n",
    "    # check that all fields are present\n",
    "    if set(fields) != set(['LEIA', 'ESCREVA']):\n",
    "        logger.error(f\"Error: {cfg_file} should have only 'LEIA' and 'ESCREVA' fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"pc.cfg\"\n",
    "logger.info(\"Reading config file \" + cfg_file)\n",
    "\n",
    "pc_read = None\n",
    "pc_queries = None\n",
    "pc_expected = None\n",
    "\n",
    "with open(os.path.join(\"config_files\", cfg_file), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    fields = [config_files[name]['field'] for name in config_files if cfg_file.endswith(name + '.cfg')][0]\n",
    "    for line in lines:\n",
    "        # check if line has format \"field=file_path\"\n",
    "        field, file_path = line.strip().split('=')\n",
    "        # if field is 'LEIA', add file_path to gli_read\n",
    "        if field == 'LEIA':\n",
    "            if pc_read != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            pc_read = file_path\n",
    "            \n",
    "        # if field is 'ESCREVA', check that it is the last field\n",
    "        elif field == 'CONSULTAS':\n",
    "            if pc_queries != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            pc_queries = file_path\n",
    "\n",
    "        elif field == 'ESPERADOS':\n",
    "            if pc_expected != None:\n",
    "                logger.error(f\"Repeated field in {cfg_file}\")\n",
    "            pc_expected = file_path\n",
    "\n",
    "        # if field is not correct, print error\n",
    "        elif field not in fields:\n",
    "            logger.error(f\"Error: {field} is not a valid field in {cfg_file}\")\n",
    "    # check that all fields are present\n",
    "    if set(fields) != set(['LEIA', 'CONSULTAS','ESPERADOS']):\n",
    "        logger.error(f\"Error: {cfg_file} adjust fields accordingly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generate inverse list read file(s): {gli_read}\")\n",
    "print(f\"Generate inverse list write file: {gli_write}\")\n",
    "print(f\"Searcher model: {busca_model}\")\n",
    "print(f\"Searcher queries: {busca_queries}\")\n",
    "print(f\"Searcher results: {busca_results}\")\n",
    "print(f\"Query processor read file: {pc_read}\")\n",
    "print(f\"Query processor query file: {pc_queries}\")\n",
    "print(f\"Query processor expected file: {pc_expected}\")\n",
    "print(f\"Index read file: {index_read}\")\n",
    "print(f\"Index write file: {index_write}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = gli_read + [gli_write] + [busca_model] + [busca_queries] + [busca_results] + [pc_read] + [pc_queries] + [pc_expected] + [index_read] + [index_write]\n",
    "for file_path in paths:\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 1 - Query Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module 1: Query processor started\")\n",
    "def process_query_text(query):\n",
    "    words = word_tokenize(query)\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    processed_query = \" \".join(words)\n",
    "    # Convert the processed query to uppercase\n",
    "    processed_query = processed_query.upper()\n",
    "\n",
    "    return processed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    query_file = pc_read\n",
    "    logger.info(\"Reading file \" + query_file)\n",
    "\n",
    "    \n",
    "    tree = ET.parse(query_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    queries = {}\n",
    "\n",
    "    for query in root.iter(\"QUERY\"):\n",
    "        query_number = int(query.find(\"QueryNumber\").text)\n",
    "        query_text = re.sub('\\s+', ' ',query.find(\"QueryText\").text.strip().replace(\";\", \" \"))\n",
    "        num_results = int(query.find(\"Results\").text)\n",
    "\n",
    "        query_text = process_query_text(query_text)\n",
    "\n",
    "        queries[query_number] = {\n",
    "            \"query_text\": query_text,\n",
    "            \"num_results\": num_results\n",
    "        }\n",
    "\n",
    "        expected_results = {}\n",
    "\n",
    "        for item in query.iter(\"Item\"):\n",
    "            expected_result = int(item.text)\n",
    "            expected_score = 0\n",
    "\n",
    "            for value in item.get(\"score\"):\n",
    "                expected_score += int(value)\n",
    "\n",
    "            expected_results[expected_result] = expected_score\n",
    "            expected_results\n",
    "        queries[query_number][\"expected_results\"] = expected_results\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_queries(queries):\n",
    "    query_data_list = []\n",
    "    \n",
    "    for query_number, query_data in queries.items():\n",
    "        expected_results = query_data[\"expected_results\"]\n",
    "\n",
    "        expected_results_str = \";\".join([f\"{doc_num}:{doc_votes}\" for doc_num, doc_votes in expected_results.items()])\n",
    "        query_data_list.append({\"QueryNumber\": query_number, \"QueryText\": query_data[\"query_text\"], \"NumResults\": query_data[\"num_results\"], \"ExpectedResults\": expected_results_str})\n",
    "\n",
    "    queries_df = pd.DataFrame(query_data_list, columns=[\"QueryNumber\", \"QueryText\"])\n",
    "\n",
    "    processed_queries_file = pc_queries\n",
    "    queries_df.to_csv(processed_queries_file, index=False, sep=\";\")\n",
    "    logger.info(\"%d queries read and written to %s.\" % (queries_df.shape[0], processed_queries_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_expected(queries):\n",
    "    logger.info(\"Writing expected results.\")\n",
    "    if len(queries.keys()) == 0:\n",
    "        logger.error(\"Method 'read_and_write_queries' must be executed first.\")\n",
    "        return\n",
    "\n",
    "    expected_results_list = []\n",
    "\n",
    "    for query_number in sorted(queries.keys()):\n",
    "        expected_results = queries[query_number][\"expected_results\"]\n",
    "        for document_number in sorted(expected_results, key = expected_results.get, reverse = True):\n",
    "            expected_results_list.append({\"QueryNumber\": query_number, \"DocNumber\": document_number, \"DocVotes\": str(expected_results[document_number])})\n",
    "    \n",
    "    expected_results_df = pd.DataFrame(expected_results_list, columns=[\"QueryNumber\", \"DocNumber\",\"DocVotes\"])\n",
    "\n",
    "    expected_results_file = pc_expected\n",
    "    expected_results_df.to_csv(expected_results_file, index=False, sep=\";\")\n",
    "    logger.info(\"%d expected results written to '%s'.\" % (expected_results_df.shape[0], expected_results_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_module_run():\n",
    "    queries = read_queries()\n",
    "    write_queries(queries)\n",
    "    write_expected(queries)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_module_run()\n",
    "logger.info(\"Module 1: Query processor finished\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 2 - Invert List Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module 2: Inverted List Generator started\")\n",
    "\n",
    "def read_input_files():\n",
    "    input_documents = {}\n",
    "\n",
    "    for path in gli_read:\n",
    "        logger.info(\"Reading file \" + path)\n",
    "\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for document in root.iter(\"RECORD\"):\n",
    "            doc_identifier = document.find(\"RECORDNUM\")\n",
    "            doc_text = document.find(\"ABSTRACT\")\n",
    "\n",
    "            if doc_identifier is not None:\n",
    "                doc_identifier = int(doc_identifier.text.strip())\n",
    "            else:\n",
    "                logger.warning(\"RECORDNUM not found for a doc in  %s\" %path)\n",
    "                continue\n",
    "            if doc_text is None:\n",
    "                doc_text = document.find(\"EXTRACT\")\n",
    "\n",
    "                if doc_text is None:\n",
    "                    logger.warning(\"Document %d in %s without ABSTRACT and EXTRACT.\" %(doc_identifier, path))\n",
    "                    continue\n",
    "\n",
    "            doc_text = doc_text.text.strip()\n",
    "\n",
    "            # logs reading document\n",
    "\n",
    "            input_documents[doc_identifier] = doc_text\n",
    "\n",
    "    return input_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inverted_list(input_documents):\n",
    "    inverted_list = {}\n",
    "    \n",
    "    for document_id in sorted(input_documents.keys()):\n",
    "        text = input_documents[document_id].upper()\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z]+') \n",
    "        words_list = tokenizer.tokenize(unidecode(text))\n",
    "\n",
    "        for word in words_list:\n",
    "            if word not in inverted_list:\n",
    "                inverted_list[word] = []\n",
    "\n",
    "            inverted_list[word].append(document_id)\n",
    "\n",
    "    logger.info(\"Inverted list generated with %d terms considered.\" %len(inverted_list))\n",
    "\n",
    "    return inverted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inverted_list(gli_write, inverted_list):\n",
    "\n",
    "    inverted_df = pd.DataFrame({\"term\": sorted(inverted_list.keys()), \"indexes\": [sorted(inverted_list[term]) for term in sorted(inverted_list.keys())]})\n",
    "    inverted_df[\"indexes\"] = inverted_df[\"indexes\"].apply(lambda x: \"[\" + \",\".join(str(num) for num in x) + \"]\")\n",
    "    inverted_df.to_csv(gli_write, sep=\";\", index=False,header = False)\n",
    "\n",
    "    total_terms = inverted_df.shape[0]\n",
    "    logger.info(\"%d terms written. Saved inverted list to '%s'\" % (total_terms, gli_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gli_module_run():\n",
    "    input_documents = read_input_files()\n",
    "    inverted_list = generate_inverted_list(input_documents)\n",
    "    save_inverted_list(gli_write, inverted_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gli_module_run()\n",
    "logger.info(\"Module 2: Generate Inverted List finished\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 3 - Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module 3: Index started\")\n",
    "\n",
    "def read_docs_dict(index_read, min_length_word=2):\n",
    "    docs_dict = {}\n",
    "\n",
    "    with open(index_read, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if len(word) >= min_length_word:\n",
    "                docs = [int(doc) for doc in row[1][1:-1].split(',')]\n",
    "                docs_dict[word] = docs\n",
    "    logger.info(\"Loading word and documents data from %s\" %index_read)\n",
    "\n",
    "    return docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_model(docs_dict):\n",
    "    docs = []\n",
    "    for doc_index in set([doc for docs in docs_dict.values() for doc in docs]):\n",
    "        doc = ' '.join([word for word, docs in docs_dict.items() if doc_index in docs])\n",
    "        docs.append(doc)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    logger.info(\"tfidf matrix created.\")\n",
    "    return tfidf_matrix, vectorizer\n",
    "    \n",
    "#dense_matrix = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(tfidf_matrix, vectorizer, index_write):\n",
    "    with open(index_write, 'wb') as f:\n",
    "        pickle.dump((tfidf_matrix, vectorizer), f)\n",
    "    logger.info(\"tfidf matrix and vectorizer saved to %s.\"%index_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_module_run():\n",
    "    docs_dict = read_docs_dict(index_read)\n",
    "    tfidf_matrix, vectorizer = get_tfidf_model(docs_dict)\n",
    "    save_model(tfidf_matrix, vectorizer, index_write)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_module_run()\n",
    "logger.info(\"Module 3: Index finished.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 4 - Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module 4: Searcher started\")\n",
    "def load_tfidf_matrix():\n",
    "    # Load the tf-idf matrix from a pickle file\n",
    "    with open(busca_model, 'rb') as f:\n",
    "        tfidf_matrix, vectorizer = pickle.load(f)\n",
    "        logger.info(\"tfidf model loaded from %s\" %busca_model)\n",
    "        return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(busca_queries):\n",
    "    queries_df = pd.read_csv(busca_queries, delimiter=';', quotechar='\"')\n",
    "    queries = dict(zip(queries_df['QueryNumber'], queries_df['QueryText']))\n",
    "    logger.info(\"Queries loaded from %s\" %busca_queries)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(tfidf_matrix, vectorizer, queries):\n",
    "    query_matrix = vectorizer.transform(queries.values())\n",
    "    query_matrix_binary = TfidfVectorizer(binary=True, vocabulary=vectorizer.vocabulary_).fit_transform(queries.values()) #binary = True so weight is 1 for all words in query\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, query_matrix_binary)\n",
    "    results = []\n",
    "\n",
    "    for query_number, query_text in queries.items():\n",
    "        similarity_scores = similarity_matrix[:, list(queries.keys()).index(query_number)]\n",
    "        sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "        for rank, index in enumerate(sorted_indices):\n",
    "            document_number = index + 1\n",
    "            similarity_distance = similarity_scores[index]\n",
    "            results.append([query_number, [rank+1, document_number, similarity_distance]])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(busca_results, results):\n",
    "    with open(busca_results, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerows(results)\n",
    "    logger.info(\"Results saved to %s\" %busca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_module_run():\n",
    "    tfidf_matrix, vectorizer = load_tfidf_matrix() \n",
    "    queries = load_queries(busca_queries)\n",
    "    results = calculate_similarity(tfidf_matrix, vectorizer, queries)\n",
    "    write_results(busca_results, results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Module 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busca_module_run()\n",
    "logger.info(\"Module 4: Searcher finished\")\n",
    "logger.info(\"Execution completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
